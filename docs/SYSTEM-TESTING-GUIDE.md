# Comprehensive Testing Guide
## How to Test the Universal Unit-Test Traceability System

**Version:** 1.0.0  
**Last Updated:** December 8, 2025  
**Purpose:** Complete testing strategy to validate every aspect of the system

---

## üìã Table of Contents

1. [Overview](#overview)
2. [Pre-Testing Checklist](#pre-testing-checklist)
3. [Phase 1: System Health Check](#phase-1-system-health-check)
4. [Phase 2: Validation Engine Testing](#phase-2-validation-engine-testing)
5. [Phase 3: Scenario-by-Scenario Testing](#phase-3-scenario-by-scenario-testing)
6. [Phase 4: Service-Specific Testing](#phase-4-service-specific-testing)
7. [Phase 5: Report Generation Testing](#phase-5-report-generation-testing)
8. [Phase 6: Edge Cases & Error Handling](#phase-6-edge-cases--error-handling)
9. [Phase 7: End-to-End Workflow Testing](#phase-7-end-to-end-workflow-testing)
10. [Test Results Documentation](#test-results-documentation)

---

## üéØ Overview

This guide provides a **systematic approach** to test every component, scenario, and workflow in the Universal Unit-Test Traceability Validator system.

### What We'll Test

‚úÖ **Validation Engine** - Core matching logic  
‚úÖ **Scenario Loading** - All format support (YAML, JSON, Markdown)  
‚úÖ **Test Parsing** - Java and TypeScript parsers  
‚úÖ **Semantic Matching** - All 7 matching strategies  
‚úÖ **Gap Detection** - Finding missing tests  
‚úÖ **Orphan Detection** - Tests and scenarios without mapping  
‚úÖ **Report Generation** - HTML, JSON, Markdown, CSV  
‚úÖ **Pre-commit Hooks** - Git integration  
‚úÖ **All 14 Customer Scenarios** - Individual validation  
‚úÖ **Error Handling** - System resilience  

---

## ‚úÖ Pre-Testing Checklist

Before starting tests, ensure the system is ready:

```bash
# 1. Verify you're in the project root
pwd
# Expected: /Users/aquibshakeel/Desktop/ai-dummy-service

# 2. Check Node.js version (should be >= 18)
node --version

# 3. Install dependencies if not done
npm install

# 4. Verify all services are present
ls -la services/
# Expected: customer-service, identity-service

# 5. Check configuration exists
cat .traceability/config.json

# 6. Verify scenario files exist
ls -la .traceability/scenarios/

# 7. Clean old reports
rm -rf .traceability/reports/*
```

**Expected State:**
- ‚úÖ Dependencies installed
- ‚úÖ Configuration file present
- ‚úÖ Scenario files exist
- ‚úÖ Test files exist in services
- ‚úÖ Clean reports directory

---

## Phase 1: System Health Check

### Test 1.1: Basic System Validation

**Purpose:** Verify the core validation system runs without errors

```bash
# Run basic validation
npm run validate

# Expected Output:
# ‚úì Loading configuration...
# ‚úì Validating services...
# ‚úì Generating reports...
```

**What to Check:**
- [ ] Command executes without errors
- [ ] Configuration loads successfully
- [ ] Services are detected
- [ ] Reports are generated

**Pass Criteria:** No errors, process completes successfully

---

### Test 1.2: Configuration Validation

**Purpose:** Ensure configuration is valid and complete

```bash
# View configuration
cat .traceability/config.json

# Validate JSON structure
node -e "console.log(JSON.parse(require('fs').readFileSync('.traceability/config.json')))"
```

**What to Check:**
- [ ] Valid JSON structure
- [ ] All required fields present
- [ ] Service paths are correct
- [ ] Matching strategies configured
- [ ] Report formats specified

**Pass Criteria:** Configuration is valid and complete

---

### Test 1.3: Dependency Check

**Purpose:** Verify all required packages are installed

```bash
# Check package.json
cat package.json

# Verify critical dependencies
npm list --depth=0

# Check for vulnerabilities
npm audit
```

**What to Check:**
- [ ] All dependencies installed
- [ ] No missing packages
- [ ] No critical vulnerabilities
- [ ] TypeScript version correct

**Pass Criteria:** All dependencies present, no critical issues

---

## Phase 2: Validation Engine Testing

### Test 2.1: Customer Service Validation

**Purpose:** Validate the customer-service (Java)

```bash
# Validate only customer-service
npm run validate -- --service customer-service

# Alternative command
./scripts/pre-commit.sh --service customer-service
```

**What to Check:**
- [ ] Service is detected
- [ ] Scenario file is loaded
- [ ] Java tests are parsed
- [ ] Scenarios are matched to tests
- [ ] Report is generated

**Expected Output:**
```
‚úì Loading scenarios from: .traceability/scenarios/customer-service.scenarios.yaml
‚úì Found 14 scenarios
‚úì Parsing Java tests from: services/customer-service/src/test/java
‚úì Found X unit tests
‚úì Matching scenarios to tests...
‚úì Coverage: XX%
```

**Pass Criteria:** Validation completes, coverage report shows percentages

---

### Test 2.2: Identity Service Validation

**Purpose:** Validate the identity-service (TypeScript)

```bash
# Validate only identity-service
npm run validate -- --service identity-service

# Alternative
./scripts/pre-commit.sh --service identity-service
```

**What to Check:**
- [ ] Service is detected
- [ ] TypeScript tests are parsed
- [ ] Tests are located correctly
- [ ] Validation completes

**Pass Criteria:** TypeScript parser works correctly

---

### Test 2.3: All Services Validation

**Purpose:** Validate all services together

```bash
# Validate all services
npm run validate --all

# Or
./scripts/pre-commit.sh --all
```

**What to Check:**
- [ ] Both services validated
- [ ] Combined statistics shown
- [ ] Total coverage calculated
- [ ] All reports generated

**Expected Output:**
```
Validating 2 services...
‚úì customer-service
‚úì identity-service
Total Coverage: XX%
```

**Pass Criteria:** All services validated successfully

---

## Phase 3: Scenario-by-Scenario Testing

### Test 3.1: CUST-001 (Get Customer Valid ID)

**Scenario:** When user requests customer with valid ID, system returns 200

```bash
# Run validation
npm run validate -- --service customer-service

# Open HTML report
open .traceability/reports/traceability-report.html

# Search for CUST-001 in report
```

**Manual Verification:**
1. Open HTML report
2. Find CUST-001 in the scenario list
3. Check status badge (should be green "Fully Covered" or yellow "Partial")
4. Verify matched test name contains "get customer" or similar
5. Check match confidence score (should be > 75%)

**What to Verify:**
- [ ] CUST-001 appears in report
- [ ] Has matched test
- [ ] Match score is reasonable
- [ ] No gap detected
- [ ] Status is "Fully Covered"

**Expected Test Match:**
```
Test: CustomerControllerTest.testGetCustomerWithValidIdReturns200()
Match Score: 85-95%
Status: ‚úÖ Fully Covered
```

---

### Test 3.2: CUST-002 (Get Customer Invalid ID)

**Scenario:** When user requests customer with invalid ID, returns 404

```bash
# Validation already ran in previous test
# Check report for CUST-002
```

**What to Verify:**
- [ ] CUST-002 in report
- [ ] Matched to 404 error test
- [ ] Keywords: "invalid", "404", "not found"
- [ ] Status: Covered or Partial

**Expected Test Match:**
```
Test: CustomerControllerTest.testGetCustomerWithInvalidIdReturns404()
Match Score: 80%+
```

---

### Test 3.3: CUST-003 (Malformed ID)

**Scenario:** Malformed ID returns 400 bad request

**What to Verify:**
- [ ] CUST-003 in report
- [ ] Check for gap or match
- [ ] If gap, verify it's reported correctly

**Action if Gap Found:**
- Document as expected behavior
- Note in test results: "Gap detected as designed"

---

### Test 3.4: CUST-004 (Authentication - P0 Priority)

**Scenario:** Request without auth token returns 401 (CRITICAL)

**What to Verify:**
- [ ] CUST-004 in report
- [ ] Priority shown as P0
- [ ] Risk Level: Critical
- [ ] **If gap exists, should BLOCK commits**

**Critical Test:**
```bash
# If CUST-004 has gap, test blocking behavior:
# Temporarily remove its test (backup first!)
# Try to commit - should be blocked

# IMPORTANT: Restore test after verification
```

---

### Test 3.5: CUST-005 (Create Customer - P1)

**Scenario:** Create customer with valid data returns 201

**What to Verify:**
- [ ] CUST-005 in report
- [ ] Match to create/POST test
- [ ] Keywords: "create", "201", "valid data"
- [ ] Acceptance criteria check

---

### Test 3.6-3.14: Remaining Scenarios

**Test Each Systematically:**

| Scenario | Description | Priority | Expected Status |
|----------|-------------|----------|-----------------|
| CUST-006 | Missing required fields ‚Üí 400 | P2 | Check coverage |
| CUST-007 | Duplicate email ‚Üí 409 | P2 | Check coverage |
| CUST-008 | Invalid email ‚Üí 400 | P2 | Check coverage |
| CUST-009 | Update customer valid data | P1 | Should be covered |
| CUST-010 | Update non-existent ‚Üí 404 | P2 | Check coverage |
| CUST-011 | Delete customer (soft delete) | P1 | Should be covered |
| CUST-012 | Delete already deleted ‚Üí 410 | P0 | **CRITICAL** |
| CUST-013 | DB connection fail ‚Üí 503 | P0 | **CRITICAL** |
| CUST-014 | DB timeout ‚Üí 504 | P0 | **CRITICAL** |

**For Each Scenario:**
1. Check if it appears in report
2. Verify match status
3. Check priority level
4. Verify acceptance criteria
5. Document coverage status

---

## Phase 4: Service-Specific Testing

### Test 4.1: Customer Service (Java/JUnit)

**Purpose:** Test Java test parser works correctly

```bash
# Check test files exist
ls -la services/customer-service/src/test/java/com/pulse/customerservice/

# Expected test files:
# - CustomerControllerTest.java
# - CustomerServiceImplTest.java
# - CustomerMapperTest.java
# etc.

# Verify they're being parsed
npm run validate -- --service customer-service --verbose
```

**What to Check:**
- [ ] Java test files are found
- [ ] @Test annotations detected
- [ ] Test names extracted
- [ ] @DisplayName honored (if present)
- [ ] Correct test count

**Manual Verification:**
```bash
# Count actual test files
find services/customer-service/src/test -name "*Test.java" | wc -l

# Count tests in validation output
# Should match or be close
```

---

### Test 4.2: Identity Service (TypeScript/Jest)

**Purpose:** Test TypeScript test parser

```bash
# Check test files
ls -la services/identity-service/test/

# Validate
npm run validate -- --service identity-service --verbose
```

**What to Check:**
- [ ] TypeScript test files found
- [ ] describe() blocks detected
- [ ] test() or it() functions found
- [ ] Test descriptions extracted

---

### Test 4.3: Test Pattern Matching

**Purpose:** Verify test patterns in config work

```bash
# Check test patterns in config
cat .traceability/config.json | grep testPattern

# For customer-service: "*Test.java"
# For identity-service: "*.test.ts"

# Verify files match patterns
find services/customer-service/src/test -name "*Test.java"
find services/identity-service/test -name "*.test.ts"
```

**Pass Criteria:** All test files match configured patterns

---

## Phase 5: Report Generation Testing

### Test 5.1: HTML Report

**Purpose:** Verify HTML report is generated and valid

```bash
# Run validation
npm run validate --all

# Check HTML report exists
ls -la .traceability/reports/traceability-report.html

# Open in browser
open .traceability/reports/traceability-report.html
```

**What to Check:**
- [ ] File exists and is not empty
- [ ] Opens in browser without errors
- [ ] All sections present:
  - [ ] Summary statistics
  - [ ] Coverage overview
  - [ ] Gap analysis
  - [ ] Orphan tests
  - [ ] Traceability matrix
  - [ ] Recommendations
- [ ] Styling works (CSS loaded)
- [ ] Interactive elements work
- [ ] Data displays correctly

**Visual Inspection:**
- Headers render properly
- Tables formatted correctly
- Color coding works (red/yellow/green)
- Progress bars show
- Priority badges visible

---

### Test 5.2: JSON Report

**Purpose:** Verify JSON report for machine readability

```bash
# Check JSON report exists
ls -la .traceability/reports/traceability-report.json

# Validate JSON structure
cat .traceability/reports/traceability-report.json | jq '.'

# Check specific fields
cat .traceability/reports/traceability-report.json | jq '.summary'
cat .traceability/reports/traceability-report.json | jq '.gaps'
cat .traceability/reports/traceability-report.json | jq '.coverage'
```

**What to Check:**
- [ ] Valid JSON format
- [ ] Contains summary object
- [ ] Contains scenarios array
- [ ] Contains tests array
- [ ] Contains gaps array
- [ ] Contains statistics

**Data Validation:**
```bash
# Verify scenario count matches
cat .traceability/reports/traceability-report.json | jq '.scenarios | length'

# Should match scenarios in YAML file
cat .traceability/scenarios/customer-service.scenarios.yaml | grep "^  - id:" | wc -l
```

---

### Test 5.3: Markdown Report

**Purpose:** Verify Markdown report for documentation

```bash
# Check Markdown report
cat .traceability/reports/traceability-report.md

# Count sections
grep "^#" .traceability/reports/traceability-report.md
```

**What to Check:**
- [ ] File exists
- [ ] Proper Markdown formatting
- [ ] Headers present
- [ ] Tables formatted
- [ ] Lists rendered
- [ ] Statistics shown

**Manual Review:**
Open in Markdown viewer or IDE to verify formatting

---

### Test 5.4: CSV Report

**Purpose:** Verify CSV for spreadsheet import

```bash
# Check CSV report
head -20 .traceability/reports/traceability-report.csv

# Verify structure
cat .traceability/reports/traceability-report.csv | head -1
# Should show: Scenario ID,Description,Status,Priority,...
```

**What to Check:**
- [ ] File exists
- [ ] Headers present
- [ ] Comma-separated values
- [ ] All scenarios listed
- [ ] Data properly escaped

**Optional:** Import into Excel/Google Sheets to verify

---

## Phase 6: Edge Cases & Error Handling

### Test 6.1: Gap Detection (P0 Priority)

**Purpose:** Verify P0 gaps block commits

**Setup:**
```bash
# Backup current scenario file
cp .traceability/scenarios/customer-service.scenarios.yaml .traceability/scenarios/customer-service.scenarios.yaml.bak

# Add a new P0 scenario without test
cat >> .traceability/scenarios/customer-service.scenarios.yaml << 'EOF'

  - id: CUST-TEST-P0
    description: Test scenario for P0 gap detection without matching test
    apiEndpoint: /api/test/p0-gap
    httpMethod: GET
    priority: P0
    riskLevel: Critical
    category: Testing
    tags: [test, gap-detection]
EOF
```

**Test Execution:**
```bash
# Run validation - should detect gap
npm run validate -- --service customer-service

# Check exit code
echo $?
# Should be non-zero (failure) if P0 gaps block
```

**What to Verify:**
- [ ] Gap detected for CUST-TEST-P0
- [ ] Reported as P0 priority
- [ ] Validation fails (if configured to block)
- [ ] Gap shown in report with red badge

**Cleanup:**
```bash
# Restore original file
mv .traceability/scenarios/customer-service.scenarios.yaml.bak .traceability/scenarios/customer-service.scenarios.yaml
```

---

### Test 6.2: Orphan Test Detection

**Purpose:** Verify system detects tests without scenarios

**Test:**
```bash
# Run validation
npm run validate --all

# Open HTML report
open .traceability/reports/traceability-report.html

# Navigate to "Orphan Tests" section
```

**What to Verify:**
- [ ] Orphan tests section exists
- [ ] Lists tests without matching scenarios
- [ ] Shows test names clearly
- [ ] Provides recommendations

**Expected:** Some orphan tests may exist (technical tests, etc.)

---

### Test 6.3: Invalid Configuration Handling

**Purpose:** Test system handles config errors gracefully

**Setup:**
```bash
# Backup config
cp .traceability/config.json .traceability/config.json.bak

# Create invalid config (remove required field)
cat .traceability/config.json | jq 'del(.services)' > .traceability/config.json.tmp
mv .traceability/config.json.tmp .traceability/config.json
```

**Test:**
```bash
# Try to run validation
npm run validate 2>&1 | tee test-output.log

# Should fail gracefully with error message
```

**What to Verify:**
- [ ] Validation fails (doesn't crash)
- [ ] Error message is clear
- [ ] Points to configuration issue
- [ ] No stack traces exposed

**Cleanup:**
```bash
# Restore config
mv .traceability/config.json.bak .traceability/config.json
```

---

### Test 6.4: Missing Scenario File

**Purpose:** Test handling of missing scenario files

**Setup:**
```bash
# Temporarily rename scenario file
mv .traceability/scenarios/customer-service.scenarios.yaml .traceability/scenarios/customer-service.scenarios.yaml.tmp
```

**Test:**
```bash
# Run validation
npm run validate -- --service customer-service 2>&1
```

**What to Verify:**
- [ ] Error detected
- [ ] Clear error message
- [ ] Doesn't crash
- [ ] Suggests creating file

**Cleanup:**
```bash
mv .traceability/scenarios/customer-service.scenarios.yaml.tmp .traceability/scenarios/customer-service.scenarios.yaml
```

---

### Test 6.5: Matching Threshold Testing

**Purpose:** Test different matching thresholds

```bash
# Test with strict threshold
# Temporarily edit config (or use environment variable if supported)
# Set defaultThreshold to 0.95 (very strict)

# Run validation and check matches
npm run validate --all

# Then test with loose threshold
# Set defaultThreshold to 0.50 (very loose)

# Compare results
```

**What to Compare:**
- [ ] Number of matches increases with lower threshold
- [ ] Quality of matches varies
- [ ] False positives at low threshold
- [ ] Missing matches at high threshold

---

## Phase 7: End-to-End Workflow Testing

### Test 7.1: Complete QA Workflow

**Purpose:** Simulate complete QA workflow

**Scenario:** QA adds new scenario, developer adds test

**Step 1: QA Adds Scenario**
```bash
# Add new scenario
cat >> .traceability/scenarios/customer-service.scenarios.yaml << 'EOF'

  - id: CUST-E2E-TEST
    description: When user exports customer data, system returns CSV file
    apiEndpoint: /api/customer/export
    httpMethod: GET
    priority: P2
    riskLevel: Medium
    category: Export
    tags: [export, csv, customer]
EOF

# Run validation
npm run validate -- --service customer-service
```

**Expected:** Gap detected for CUST-E2E-TEST

**Step 2: Check Gap in Report**
```bash
open .traceability/reports/traceability-report.html
# Verify CUST-E2E-TEST shows as "Not Covered"
```

**Step 3: Developer Adds Test** (Simulation)
```bash
# In real scenario, developer would write actual test
# For testing, we'll just verify the system would detect it

# Check JSON report for gap
cat .traceability/reports/traceability-report.json | jq '.gaps[] | select(.scenarioId == "CUST-E2E-TEST")'
```

**Step 4: Cleanup**
```bash
# Remove test scenario
# Edit file to remove CUST-E2E-TEST manually
```

---

### Test 7.2: Git Pre-Commit Hook

**Purpose:** Test pre-commit hook integration

**Warning:** Only do this if git hooks are installed

```bash
# Check if hooks installed
ls -la .git/hooks/pre-commit

# If not installed
npm run install:hooks

# Make a trivial change to test
echo "# Test comment" >> README.md

# Try to commit
git add README.md
git commit -m "Test pre-commit hook"

# Pre-commit should run validation
```

**What to Verify:**
- [ ] Hook executes automatically
- [ ] Validation runs
- [ ] If validation fails, commit blocked
- [ ] If validation passes, commit succeeds

**Cleanup:**
```bash
# Reset the test change
git reset HEAD~1
git checkout README.md
```

---

### Test 7.3: CI/CD Simulation

**Purpose:** Simulate CI/CD pipeline execution

```bash
# Clean environment
rm -rf node_modules .traceability/reports/*

# Fresh install
npm install

# Run validation (as CI would)
npm run validate --all

# Check exit code
echo $?

# Verify reports generated
ls -la .traceability/reports/
```

**What to Verify:**
- [ ] Works in clean environment
- [ ] No dependency issues
- [ ] Reports generated
- [ ] Exit code correct (0 = pass, non-zero = fail)

---

## Test Results Documentation

### Test Results Template

Create a test results file to track your findings:

```bash
# Create test results file
cat > TEST-RESULTS.md << 'EOF'
# Test Results - Universal Unit-Test Traceability Validator

**Test Date:** [Date]
**Tester:** [Your Name]
**System Version:** 2.0.0

## Summary

| Phase | Tests Passed | Tests Failed | Status |
|-------|-------------|--------------|--------|
| Phase 1: System Health | 0/3 | 0 | ‚è≥ Not Started |
| Phase 2: Validation Engine | 0/3 | 0 | ‚è≥ Not Started |
| Phase 3: Scenarios | 0/14 | 0 | ‚è≥ Not Started |
| Phase 4: Services | 0/3 | 0 | ‚è≥ Not Started |
| Phase 5: Reports | 0/4 | 0 | ‚è≥ Not Started |
| Phase 6: Edge Cases | 0/5 | 0 | ‚è≥ Not Started |
| Phase 7: E2E Workflows | 0/3 | 0 | ‚è≥ Not Started |

## Detailed Results

### Phase 1: System Health Check

#### Test 1.1: Basic System Validation
- Status: [ ] Pass [ ] Fail
- Notes: 
- Issues Found:

#### Test 1.2: Configuration Validation
- Status: [ ] Pass [ ] Fail
- Notes:
- Issues Found:

[Continue for all tests...]

## Issues Found

1. **Issue #1:**
   - Severity: Critical/High/Medium/Low
   - Description:
   - Steps to Reproduce:
   - Expected vs Actual:

## Recommendations

1. 
2.
3.

## Coverage Analysis

- Total Scenarios: 14
- Fully Covered: 
- Partially Covered:
- Not Covered (Gaps):
- Overall Coverage: ___%

## Conclusion

[ ] System is production-ready
[ ] System needs minor fixes
[ ] System needs major fixes
[ ] System is not ready

EOF

# Now fill in results as you test
```

---

## Quick Test Commands Reference

```bash
# === BASIC TESTING ===
npm run validate                              # Validate all
npm run validate -- --service customer-service  # Validate one service

# === REPORTS ===
open .traceability/reports/traceability-report.html   # View HTML
cat .traceability/reports/traceability-report.json    # View JSON
cat .traceability/reports/traceability-report.md      # View Markdown

# === DEBUGGING ===
npm run validate -- --verbose                 # Verbose output
npm run validate -- --service customer-service --verbose  # Service verbose

# === MANUAL EXECUTION ===
./scripts/pre-commit.sh --all                # Manual all services
./scripts/pre-commit.sh --service customer-service  # Manual one service

# === CHECK FILES ===
ls -la services/customer-service/src/test/java  # Check Java tests
ls -la services/identity-service/test          # Check TS tests
ls -la .traceability/scenarios/                # Check scenarios
cat .traceability/config.json                  # View config

# === CLEANUP ===
rm -rf .traceability/reports/*               # Clean reports
npm run clean                                # Clean (if script exists)
```

---

## Test Execution Order

**Recommended order for systematic testing:**

1. ‚úÖ **Start with Phase 1** - Verify system basics work
2. ‚úÖ **Then Phase 2** - Test validation engine
3. ‚úÖ **Then Phase 3** - Test each scenario individually
4. ‚úÖ **Then Phase 4** - Test service-specific features
5. ‚úÖ **Then Phase 5** - Verify all reports
6. ‚úÖ **Then Phase 6** - Test edge cases
7. ‚úÖ **Finally Phase 7** - Complete E2E workflows

**Time Estimate:**
- Phase 1: 15 minutes
- Phase 2: 20 minutes
- Phase 3: 45 minutes (14 scenarios)
- Phase 4: 20 minutes
- Phase 5: 25 minutes
- Phase 6: 30 minutes
- Phase 7: 25 minutes
- **Total: ~3 hours** for comprehensive testing

---

## Success Criteria

‚úÖ **System passes if:**
- All phases complete without critical errors
- Reports generate correctly
- Scenario matching works (>70% coverage)
- P0 scenarios are covered or block appropriately
- Edge cases handled gracefully
- Documentation matches actual behavior

‚ö†Ô∏è **Acceptable issues:**
- Some P2/P3 gaps (by design)
- Some orphan tests (technical tests)
- Minor formatting issues in reports

‚ùå **Fails if:**
- Validation crashes
- Reports don't generate
- P0 scenarios not covered and not blocked
- Configuration errors not handled
- Major functionality broken

---

## Next Steps After Testing

1. **Document all findings** in TEST-RESULTS.md
2. **Create JIRA tickets** for issues found
3. **Update documentation** if gaps found
4. **Share test report** with team
5. **Prioritize fixes** based on severity
6. **Retest after fixes** applied

---

**Happy Testing! üéØ**
